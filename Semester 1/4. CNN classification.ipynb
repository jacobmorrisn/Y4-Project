{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JakeM\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from astropy.io import fits \n",
    "from astropy import stats\n",
    "from scipy.ndimage import rotate\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clipped_images(filepath,xpix,ypix,sigma):\n",
    "    '''\n",
    "    Put FITS data from desired folder into a 3D array\n",
    "    sigma = how many sigmas from the median background value to sigma clip the data to\n",
    "    \n",
    "    TODO: Need to check for .fits ending so works if other files in directory\n",
    "    '''\n",
    "    newpath = filepath.replace(os.sep, '/')\n",
    "    dirs = os.listdir(newpath)\n",
    "    n = len(dirs)\n",
    "    data = np.empty(shape=(n,xpix,ypix),dtype=np.float64)\n",
    "    for i in range(n):\n",
    "        fullpath = '{}/{}'.format(newpath,dirs[i])\n",
    "        d = fits.getdata(fullpath, ext=0)\n",
    "        d[np.isnan(d)] = 0\n",
    "        _,median,std = stats.sigma_clipped_stats(d, sigma=sigma)\n",
    "        d[d<median+sigma*std] = median+sigma*std\n",
    "        data[i,:,:] = d\n",
    "        #if i%100==0:\n",
    "            #print(i)\n",
    "    return data\n",
    "\n",
    "def augment_data(data,size,xpix,ypix):\n",
    "    '''\n",
    "    Augment the data (3D array of images) by flipping and rotating the images.\n",
    "    Size = upper bound on the final number of images \n",
    "    (actual_size can be much less depending on size/data_size multiples)\n",
    "    \n",
    "    TODO: Make the actual size = size\n",
    "    Make sure still works if size<len(data)\n",
    "    '''\n",
    "    rotations = size//len(data) # rotations per image\n",
    "    angles = np.linspace(0, 360, rotations)\n",
    "    act_size = rotations*len(data)\n",
    "    training_set = np.empty((act_size, xpix, ypix))\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(angles)):\n",
    "            if j % 2 == 0: training_set[i*len(angles)+j,:,:] = rotate(np.fliplr(data[i,:,:]), angles[j], reshape=False)\n",
    "            else: training_set[i*len(angles)+j,:,:] = rotate(data[i,:,:], angles[j], reshape=False)\n",
    "    return training_set\n",
    "\n",
    "def train_test(data,percentage):\n",
    "    '''\n",
    "    Combines data sets in one 3D array, with a different label for each data set.\n",
    "    Then randomly shuffles the data and splits into training and test sets.\n",
    "    data = list of 3D arrays containing desired data sets\n",
    "    per = fraction of data to be in training set\n",
    "    returns: train and test data (each a tupple containing the data and corresponding labels)\n",
    "    '''\n",
    "    d = np.concatenate(data,axis=0)\n",
    "    n_images = len(d)\n",
    "    labels = np.empty(n_images)\n",
    "    i = 0\n",
    "    for n in range(len(data)):\n",
    "        labels[i:i+len(data[n])] = n\n",
    "        i += len(data[n])\n",
    "    rand_ind = np.random.permutation(range(n_images))\n",
    "    d, labels = d[rand_ind], labels[rand_ind]\n",
    "    n_train = np.int(np.round(n_images*percentage))\n",
    "    train = (d[:n_train], labels[:n_train])\n",
    "    test = (d[n_train:], labels[n_train:])\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of x pixels and y pixels in each image (must be the same for all images)\n",
    "xpix, ypix = 83, 83\n",
    "\n",
    "# Directories with the FITS data\n",
    "agn_path = r'C:\\Users\\JakeM\\Google Drive\\University of Birmingham\\Year 4\\Project\\5. Code\\Data\\FITS agn'\n",
    "ps_path = r'C:\\Users\\JakeM\\Google Drive\\University of Birmingham\\Year 4\\Project\\5. Code\\Data\\FITS stars'\n",
    "\n",
    "# Get FITs images and augment data sets\n",
    "agn_data = get_clipped_images(agn_path,xpix,ypix,sigma=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('agn_data', agn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_data = get_clipped_images(ps_path,xpix,ypix,sigma=1)\n",
    "agn_augment = augment_data(agn_data,10000,xpix,ypix)\n",
    "ps_augment = augment_data(ps_data,10000,xpix,ypix)\n",
    "\n",
    "data = (agn_augment, ps_augment) # list of all data sets to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test(data,0.2) # training and test data, each in a tuple with data label array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(xpix, ypix)),\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dropout(0.3),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
    "    \n",
    "# Compile the network\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3483 samples\n",
      "Epoch 1/10\n",
      "3483/3483 [==============================] - 6s 2ms/sample - loss: 0.7933 - accuracy: 0.8200\n",
      "Epoch 2/10\n",
      "3483/3483 [==============================] - 3s 872us/sample - loss: 0.0433 - accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "3483/3483 [==============================] - 3s 845us/sample - loss: 0.0118 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "3483/3483 [==============================] - 3s 883us/sample - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "3483/3483 [==============================] - 3s 876us/sample - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "3483/3483 [==============================] - 3s 881us/sample - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "3483/3483 [==============================] - 3s 891us/sample - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "3483/3483 [==============================] - 3s 895us/sample - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "3483/3483 [==============================] - 3s 922us/sample - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "3483/3483 [==============================] - 3s 931us/sample - loss: 8.8564e-04 - accuracy: 1.0000\n",
      "13931/1 - 4s - loss: 0.0017 - accuracy: 0.9997\n",
      "\n",
      "Test accuracy: 0.9997129\n"
     ]
    }
   ],
   "source": [
    "# Perform fit\n",
    "model.fit(*train, epochs=10)\n",
    "test_loss, test_acc = model.evaluate(*test, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above shows can achieve 100% accruacy between point sources and AGN even with a simple neural net.\n",
    "\n",
    "### Classifying AGN with jet angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3723, 3712)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot histogram of the angles of the AGN, using data given in VizieR data catalog\n",
    "# Need to get txt file with the angle in it, similar to RA and Dec txt file\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "path = r'C:\\Users\\JakeM\\Google Drive\\University of Birmingham\\Year 4\\Project\\5. Code\\Data\\agn bending angle.txt'\n",
    "agn_ang = np.array(pd.read_csv(path, sep='\\s+', header=None, usecols=[2])) # bending angle of agn, from vizier catalog\n",
    "len(agn_ang), len(agn_data)\n",
    "\n",
    "# Not the same length since some coords couldn't be downloaded... 'out of range of survey'\n",
    "# Need to be the same length for labelling...\n",
    "# So compare GaronAGN2019_RA_dec.txt coords with filename or coords in fits file to find which are missing\n",
    "# Or use download form..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have to use RA in degrees (i.e. decimal) for correct comparison and rounding\n",
    "\n",
    "path = r'C:\\Users\\JakeM\\Google Drive\\University of Birmingham\\Year 4\\Project\\5. Code\\Data\\GaronAGN2019_RA_dec.txt'\n",
    "agn_ra = np.squeeze(np.array(pd.read_csv(path, sep='\\s+', header=None, usecols=[1])))\n",
    "agn_ra = np.around(agn_ra, 5)\n",
    "\n",
    "directory_path = r'C:\\Users\\JakeM\\Google Drive\\University of Birmingham\\Year 4\\Project\\5. Code\\Data\\FITS agn'\n",
    "files = [f for f in os.listdir(directory_path) if f.endswith('.fits')]\n",
    "\n",
    "files_ra = np.empty(len(files))\n",
    "for i in range(len(files)):\n",
    "    file_path = directory_path + '\\\\' + files[i]\n",
    "    ra = fits.getval(file_path, 'OBJCTRA')\n",
    "    hours, mins, secs = int(ra[:2]), int(ra[3:5]), float(ra[6:12])\n",
    "    ra_deg = (hours+mins/60+secs/3600)*(360/24) # convert RA from hh:mm:ss to degrees\n",
    "    ra_deg = round(ra_deg, 5)\n",
    "    files_ra[i] = ra_deg\n",
    "    \n",
    "missing_i = np.squeeze([np.argwhere(agn_ra==ra) for ra in agn_ra if ra not in files_ra])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3712, 3712)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agn_ang = np.delete(agn_ang, missing_i)\n",
    "len(agn_ang), len(agn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('agn_ang', agn_ang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.hist(agn_ang, bins = 100, rwidth = 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define angle ranges for three different categories of AGN (choose limits to split data in thirds??)\n",
    "ang1 = 20\n",
    "ang2 = 45\n",
    "\n",
    "# Assign each image in agn_data a label depending on which angle range it is in\n",
    "labels = np.empty(len(agn_data))\n",
    "for i in range(len(agn_ang)):\n",
    "    if agn_ang[i] < ang1: labels[i] = 1\n",
    "    elif agn_ang[i] > ang2: labels[i] = 3\n",
    "    else: labels[i] = 2\n",
    "\n",
    "# Could achieve this in the above way with the original agn_data array retained, or seperate into 3 arrays for each cat.\n",
    "# Need to augment the data and have a label array for the augmented array.\n",
    "# Easy to achieve using defined functions if split into 3 arrays\n",
    "# This would also allow augmenting of each category individually which has benefits.\n",
    "\n",
    "# Was thinking could be more efficient to keep in array and augment along with labels\n",
    "# This would mean no need to concatenate the arrays of each data set / category..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat1 = agn_data[labels==1]\n",
    "cat2 = agn_data[labels==2]\n",
    "cat3 = agn_data[labels==3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment the data\n",
    "data = [] # empty list\n",
    "for cat in (cat1,cat2,cat3):\n",
    "    data.append(augment_data(cat,10000,xpix,ypix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and test data and labels\n",
    "train, test = train_test(data, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(xpix, ypix)),\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dropout(0.3),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
    "    \n",
    "# Compile the network\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22334 samples\n",
      "Epoch 1/10\n",
      "22334/22334 [==============================] - 22s 989us/sample - loss: 0.8556 - accuracy: 0.5868\n",
      "Epoch 2/10\n",
      "22334/22334 [==============================] - 21s 921us/sample - loss: 0.6980 - accuracy: 0.6581\n",
      "Epoch 3/10\n",
      "22334/22334 [==============================] - 21s 922us/sample - loss: 0.6549 - accuracy: 0.7103\n",
      "Epoch 4/10\n",
      "22334/22334 [==============================] - 21s 924us/sample - loss: 0.5989 - accuracy: 0.7710\n",
      "Epoch 5/10\n",
      "22334/22334 [==============================] - 21s 922us/sample - loss: 0.5389 - accuracy: 0.8153\n",
      "Epoch 6/10\n",
      "22334/22334 [==============================] - 21s 919us/sample - loss: 0.4809 - accuracy: 0.8465\n",
      "Epoch 7/10\n",
      "22334/22334 [==============================] - 21s 934us/sample - loss: 0.4339 - accuracy: 0.8625- loss: 0.4350 - \n",
      "Epoch 8/10\n",
      "22334/22334 [==============================] - 21s 918us/sample - loss: 0.3965 - accuracy: 0.8727\n",
      "Epoch 9/10\n",
      "22334/22334 [==============================] - 21s 919us/sample - loss: 0.3697 - accuracy: 0.8814\n",
      "Epoch 10/10\n",
      "22334/22334 [==============================] - 21s 921us/sample - loss: 0.3433 - accuracy: 0.8911\n",
      "5584/1 - 2s - loss: 0.2630 - accuracy: 0.8825\n",
      "\n",
      "Test accuracy: 0.8825215\n"
     ]
    }
   ],
   "source": [
    "# Perform fit\n",
    "model.fit(*train, epochs=10)\n",
    "test_loss, test_acc = model.evaluate(*test, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
